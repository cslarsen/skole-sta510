\documentclass[a4paper,english,12pt]{article}
\input{preamble}

\title{STA510 --- Statistical modelling and simulation}
\subtitle{Exercise Set 1}
\author{Christian Stigen}
\date{UiS, September \nth{21}, 2017}

\begin{document}
\maketitle

\problem{1 (a)}
The probability of getting \textit{any specific} sequence is exactly the same,
because if we list all possible outcomes for $n$ flips, each sequence will
appear exactly once.

It is understood that each flip is stochastic, meaning there is no memory
involved. Thus, each flip is independent of any other flips. That the coin is
unbiased means that the probability for getting heads or tails is exactly
$0.5$, i.e.~the outcome is both binary and evenly distributed. In fact, this is
called a \textit{Bernoulli process}, and each flip is a \textit{Bernoulli
trial}.

In mathematical terms, we have that
\begin{align*}
  \Prb(\textrm{HHTHT}) = \Prb(\textrm{THHHT}) = \Prb(\textrm{five flips}) =
    \left(\frac{1}{2}\right)^5 = \frac{1}{32}
\end{align*}
Conversely, we see that there are $2^5 = 32$ possible ways to toss a fair coin
five times.

\problem{1 (b)}
I modified the lecture code to simulate the number of heads in five coin flips.
The relative frequencies for different number of simulations are shown in
figure \vref{plot.1b}.

As the number of simulations increase, we can see that the plot seem to
approximate the normal distribution. This is what we expect from a discrete,
Binomial distribution.
\begin{figure}[H]
  \includegraphics[width=\textwidth]{code1/problem-1b.pdf}
  \caption{Plot for problem 1 (b) on page \pageref{problem.1 (b)}.}
  \label{plot.1b}
\end{figure}

\problem{1 (c)}
I added the keyword argument \texttt{prob=c(0.4, 0.6)} to the \texttt{sample}
function. That makes the probabilities for getting heads and tails 0.6 and 0.4,
correspondingly.

The resulting plots can be seen in figure \vref{plot.1c}. We clearly see that
for a large number of samples, the plots are skewed to the right, as we expect.
\begin{figure}[H]
  \includegraphics[width=\textwidth]{code1/problem-1c.pdf}
  \caption{Plot for problem 1 (c) on page \pageref{problem.1 (c)}.}
  \label{plot.1c}
\end{figure}

\problem{2 (a)}
The definition for a probability mass function (PMF) require that
\begin{align}
  f(x) &\geqslant 0 \label{pmf.1} \\
  \sum_x{f(x)} &= 1 \label{pmf.2} \\
  f(x) &= \Prb(X=x) \label{pmf.3}
\end{align}
For $f(x) = x/c$ with $x=1,2,3,4$, we see that requirement (\ref{pmf.1}) is met.
But for (\ref{pmf.2}) and (\ref{pmf.3}), we must have
$\sfrac{1}{c} + \sfrac{2}{c} + \sfrac{3}{c} + \sfrac{4}{c} = \sfrac{10}{c}$
or $c=10$.

The bar plot can be seen in figure \vref{plot.2a}.
\begin{figure}[H]
  \includegraphics[width=\textwidth]{code1/problem-2a.pdf}
  \caption{Plot for problem 2 (a) on page \pageref{problem.2 (a)}.}
  \label{plot.2a}
\end{figure}

\problem{2 (b)}
For $f(x) = c(x+1)^2$ with $x=0,1,2,3$ we have the sum $c+4c+9c+16c=30c$.  To
make the sum one, we must therefore have $c=\frac{1}{30}$. All other
requirements are then met.

The bar plot can be seen in figure \vref{plot.2b}.
\begin{figure}[H]
  \includegraphics[width=\textwidth]{code1/problem-2b.pdf}
  \caption{Plot for problem 2 (b) on page \pageref{problem.2 (b)}.}
  \label{plot.2b}
\end{figure}

\problem{3}
Since we have a finite number of values, the expected value $\E(X)$ is given by
\[
  \E(X) = x_1p_1 + x_2p_2 + \cdots + x_kp_k
\]
where $x_n$ is the outcome of the random variable, while $p_n$ is its
probability. Since we already know $c$ in both cases, we can just plug the
numbers in.

\paragraph{The expected value for problem 2 (a)} is given by
\[
  \E(X) =
      1\frac{1}{c} +
      2\frac{2}{c} +
      3\frac{3}{c} +
      4\frac{4}{c} +
      = \frac{30}{c}
\]
With $c=10$ we then get $\E(X) = 3$.

Likewise, $\E(g(X))$ with $g(x)=x^3$ and $c=10$ gives
\begin{align*}
  \E(g(X)) &= \sum_x{g(x)f(x)} =
    1^3\frac{1}{10} +
    2^3\frac{2}{10} +
    3^3\frac{3}{10} +
    4^3\frac{4}{10} =
    \frac{354}{10} = 35.4
\end{align*}

\paragraph{The expected value for problem 2 (b)} is given by
\begin{align*}
  \E(X) &=
    0 +
    1 c(1+1)^2 +
    2 c(2+1)^2 +
    3 c(3+1)^2 =
    4c + 18c + 48c = 70c \\
    &= \frac{70}{30} \approx 2.33
\end{align*}
For $\E(g(X))$ we get
\begin{align*}
  \E(g(X)) &=
    0 +
    1^3 c(1+1)^2 +
    2^3 c(2+1)^2 +
    3^3 c(3+1)^2 \\
    &=  4c + 72c + 432c = \frac{508}{30} \approx 16.93
\end{align*}
I wrote an R simulation to verify the above numbers. It repeatedly draws
weighted samples and computes their average. The source is in figure
\vref{code.3} and an example run is given in figure \vref{output.3}.

While the simulation runs slower than the exact calculation for this example,
it will run \textit{vastly} faster for large, complex and computationally
expensive probability functions.
\begin{figure}[H]
  \verbatiminput{code1/problem-3.R}
  \caption{R program for simulating the expected value of a PMF (problem 3).}
  \label{code.3}
\end{figure}
\begin{figure}[H]
  \verbatiminput{code1/problem-3.out}
  \caption{Example output from the program in figure \vref{code.3}.}
  \label{output.3}
\end{figure}

\problem{4 (a)}
We can calculate $\Prb(X < 0.5)$ from first principles by using its definition
\begin{equation}
  \label{eq.4a}
  \Prb(a < X < b) = \int_b^x{f(x)\, \mathrm{d} x}
\end{equation} and setting $a=-\infty$ and $b=0.5$, or
\begin{align*}
  \Prb(X < 0.5) &=
    \int_0^{0.5} 4x(1-x^2)\, \mathrm{d} x =
    \int_0^{0.5} 4x\, \mathrm{d} x - \int_0^{0.5} 4x^3\, \mathrm{d} x \\
  &=
    \left[2x^2 - x^4 \right]_0^{0.5}
  = \frac{1}{2} - \frac{1}{16} = \frac{7}{16} = 0.4375
\end{align*}
\paragraph{Normalization check}
\[
  \int_{-\infty}^{\infty} f(x)\, \mathrm{d} x =
    \left[ 2x^2 - x^4 \right]_{0}^{1} = 2 - 1 = 1
\]%
Since the area is $1$ and $f(x) \geqslant 0$ for all
$x$, we don't need to normalize $\Prb(X)$.

\paragraph{Expected value}
\[
  \E(X)
    = \int_{-\infty}^{\infty} x\,4x(1-x^2)\, \mathrm{d} x
    = \int_{0}^{0.5} 4x^2 - 4x^4 \, \mathrm{d} x
    = \left[ \frac{4}{3}x^3 - \frac{4}{5}x^5 \right]_{0}^{1}
    = \frac{8}{15} = 0.5333\dots
\]
\paragraph{Variance}
\begin{align*}
  \E(X^2)
    = \int_{-\infty}^{\infty} x^2\,4x(1-x^2)\, \mathrm{d} x
    = \int_{0}^{0.5} 4x^3 - 4x^5 \, \mathrm{d} x
    = \left[ x^4 - \frac{4}{6}x^6 \right]_{0}^{1}
    = 1 - \frac{4}{6} = \frac{1}{3}
\end{align*}
which gives us
\begin{align*}
  \Var(X) = \E(X^2) - \E(X)^2 = \frac{1}{3} - \frac{8^2}{15^2} =
    \frac{11}{225} = 0.04888\dots
\end{align*}
\paragraph{Standard deviation}\[
  \SD(X) = \sqrt{\Var(X)} = \sqrt{\frac{11}{225}} = \frac{\sqrt{11}}{15}
  \approx 0.2211
\]

\problem{4 (b)}
The cumulative distribution function (CDF) is given by
\[
  F(X) = \Prb(X \leqslant x) = \int_{-\infty}^{x} f(t)\, \mathrm{d}t
    = \left[ 2t^2 - t^4 \right]_{0}^{x} = 2x^2 - x^4
\]
$\boldsymbol{\Prb(X \leqslant 0.3)}$ is found by using $F(0.3)$.
\[
  \Prb(X \leqslant 0.3) = F(0.3) = 2\cdot0.3^2 - 0.3^4 = 0.18 - 0.0081 = 0.1719
\]
The reason $F$ gives us the probability of $X$ being less than \textit{or equal
to} $x$ is because the point probability $\Prb(x) = 0$ for continuous
distributions.

$\boldsymbol{\Prb(X > 0.7)}$ is given by
\[
  1 - \Prb(X \leqslant 0.7) = 1 - F(0.7)
    = 1 - (2\cdot 0.7^2 - 0.7^4) = 0.2601
\]

\problem{4 (c)}
A plot of $f$ and $F$ can be seen in figure \vref{plot.4c}.
  \begin{figure}[H]
  \includegraphics[width=\textwidth]{code1/problem-4c.pdf}
  \caption{Plots for problem 4 (c) on page \pageref{problem.4 (c)}.}
  \label{plot.4c}
\end{figure}

\problem{5 (a)}
To determine $k$, we use the fact that any probability density function (PDF)
must have an area of exactly one (of course without any negative
probabilities). Since $f(x) = 0$ for $|x| > 1$, we only have to solve
\begin{align*}
  \int_{-1}^{1}k(1-x^2)\, \mathrm{d}x =
  k\left[ x - \frac{1}{3}{x^3} \right]_{-1}^{1} &=
  k\left( 1 - \frac{1}{3} + 1 - \frac{1}{3} \right) = \frac{4}{3}k = 1 \\
   k &= \frac{3}{4}
\end{align*}
A plot can be seen in figure \vref{plot.5a}.
\begin{figure}[H]
  \includegraphics[width=\textwidth]{code1/problem-5a.pdf}
  \caption{Plot for problem 5 (a) on page \pageref{problem.5 (a)}.}
  \label{plot.5a}
\end{figure}

\problem{5 (b)}
\[
  \Prb(X \leqslant 0.5) = 
    \int_{-\infty}^{0.5} f(x)\, \mathrm{d}x =
  \frac{3}{4}\left[ x - \frac{1}{3}{x^3} \right]_{-1}^{0.5} =
    0.34375 + 0.5 = 0.84375
\]

\problem{5 (c)}\begin{align*}
  \E(Y) &=
    \int_{-\infty}^{\infty} (1+x^2)\, f(x)\, \mathrm{d}x =
    k\int_{-1}^{1} (1+x^2)(1-x^2) \, \mathrm{d}x =
    k\int_{-1}^{1} 1 - x^4 \, \mathrm{d}x \\ &=
    k\left[ x - \frac{1}{5} x^5 \right]_{-1}^{1} =
    k\left( 1 - \frac{1}{5} + 1 - \frac{1}{5} \right) = \frac{3}{4}\cdot\frac{5}{8}
      = \frac{15}{32} = 0.46875
\end{align*}

\problem{6 (a)}
A scatterplot can be found in figure \vref{plot.6a}.
\begin{figure}[H]
  \includegraphics[width=\textwidth]{code1/problem-6.pdf}
  \caption{Plot for problem 6 (a) on page \pageref{problem.6 (a)}.}
  \label{plot.6a}
\end{figure}

\problem{6 (b)}
By using \texttt{cor(x, y)}, the result is $0.9431302$. This means that there
is a very high correlation between the two gas station's prices.

It also means that the correlation is on the positive side, meaning that they
are correlated the same way. That is, they are not negatively correlation so
that when one station's price is high, the other is low. Here they are both
high or both low.

In practical terms, the two stations are probably pretty close by. I would
expect two stations the are far apart in the same city to have less correlated
prices. This is because they serve different groups of customers. E.g., a good
location with a lot of passing cars and an easy access may permit higher
prices, while a small station may need to lower prices to attract customers.

But we should not read too much into the correlation value, except that these
prices are very similar on the same days.

\problem{7 (a)}
\problem{7 (b)}

\problem{8}

\problem{9 (a)}
\problem{9 (b)}

\problem{10 (a)}
\texttt{\input{code1/problem-10a.R}} gives
\texttt{\input{code1/problem-10a.out}}

\problem{10 (b)}
\texttt{\input{code1/problem-10b.R}} gives
\texttt{\input{code1/problem-10b.out}}

\problem{10 (c)}
\verbatiminput{code1/problem-10c.R} gives
\texttt{\input{code1/problem-10c.out}}

\problem{10 (d)}
\texttt{\input{code1/problem-10d.R}} gives
\texttt{\input{code1/problem-10d.out}}

\problem{11 (a)}

\end{document}
