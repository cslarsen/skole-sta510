\documentclass[a4paper,english,12pt]{article}
\input{preamble}

\title{STA-510 Statistical modelling and simulation}
\subtitle{Mandatory exercise 2}
\author{Christian Stigen}
\date{UiS, October \nth{19}, 2017}

\begin{document}
\maketitle
\section*{\normalsize{How to run the code}}
All the code for this exercise can be found in \texttt{assignment-2.R}. I have
made a single function for each problem. For example, to see the output of
problem 1 (d), execute the function \texttt{problem1d()}.  From the UNIX
command-line, you can run
\begin{verbatim}
Rscript -e 'pdf("problem1b.pdf");
            source("assignment-2.R");
            problem1b()' > problem1b.out
\end{verbatim}
to produce the textual output file \texttt{problem1b.out} and any plots in
\texttt{problem1b.pdf}. In R Studio, you may want to reset the display with
\texttt{par(mfrow=c(1,1))} before each call. All plots and R output was
generated automatically at the same time as this document.

\problem{1 (a)}
\label{problem.1a}
Because
\begin{align*}
  \rho_{ab} &= \frac{\Cov(X_a, X_b)}{\sqrt{\Var(X_a)\Var(X_b)}} =
    \frac{\sigma_{ab}}{\sigma_a\sigma_b} \\
\end{align*}
we have that $\sigma_{ab} = \rho_{ab}\sigma_a\sigma_b$ and $\sigma_{aa} = 1$.
Furthermore, by definition $\Cov(X,X) = \E(X^2) - \E(X)^2 = \Var(X)$,
so that $\rho_{aa} = 1$ and $\sigma_{aa} = \Var(X_a) = \sigma_a^2$.

The \textbf{covariance matrix} is then
\[
  \Sigma\left(\textbf{X}\right) =
    \begin{bmatrix}
      \rho_{11} & \rho_{12} & \rho_{13} \\
      \rho_{21} & \rho_{22} & \rho_{23} \\
      \rho_{31} & \rho_{32} & \rho_{33} \\
    \end{bmatrix}
  =
    \begin{bmatrix}
      900 & -0.8 \cdot 30 \cdot 10 & 0.2 \cdot 30 \cdot 4 \\
      \rho_{12} & 100 & -0.3 \cdot 10 \cdot 4 \\
      \rho_{13} & \rho_{23} & 16 \\
    \end{bmatrix}
  =
    \begin{bmatrix}
       900 & -240 &  24 \\
      -240 &  100 & -12 \\
        24 &  -12 &  16 \\
    \end{bmatrix}
\]
while the \textbf{expectation vector} is
\[
  \bm{\mu} = \left( \mu_1 , \mu_2, \mu_3 \right)^T 
    = \left( 90, 48, 18 \right)^T
\]

\problem{1 (b)}
Output from R using \texttt{rmvnorm} \cite{mvtnorm}:
\VerbatimInput{problem1b.out}
We could also use the function \texttt{pmvnorm} to calculate the first two:
\VerbatimInput{problem1b_alternative.out}

\problem{1 (c)}
A top-tier character demands that \text{both} $X_1$ and $X_2$ are high at the
same time. Therefore, we should have the highest probability of getting a
top-tier character if they are positively correlated, so that a high $X_1$
implies a high $X_2$ and vice-versa.

\paragraph{(i)   $\rho_{12} =  -0.8$, $\rho_{13} = \rho_{23} = 0$}
\[
  \Sigma\left(\textbf{X}\right) =
    \begin{bmatrix}
      \rho_{11} & \rho_{12} & \rho_{13} \\
      \rho_{21} & \rho_{22} & \rho_{23} \\
      \rho_{31} & \rho_{32} & \rho_{33} \\
    \end{bmatrix}
  =
    \begin{bmatrix}
                         900 & -0.8 \cdot 30 \cdot 10 &    0 \\
      -0.8 \cdot 10 \cdot 30 &                    100 &    0 \\
                           0 &                      0 &   16 \\
    \end{bmatrix}
  =
    \begin{bmatrix}
                         900 &                   -240 &    0 \\
                        -240 &                    100 &    0 \\
                           0 &                      0 &   16 \\
    \end{bmatrix}
\]
This scenario should have the \textit{least probability} of a top-tier
character, because $X_1$ and $X_2$ are correlated negatively: When one is high,
the other tends to be low.

\paragraph{(ii)  $\rho_{12} =   0$, $\rho_{13} = \rho_{23} = 0$}
\[
  \Sigma\left(\textbf{X}\right) =
    \begin{bmatrix}
      \rho_{11} & \rho_{12} & \rho_{13} \\
      \rho_{21} & \rho_{22} & \rho_{23} \\
      \rho_{31} & \rho_{32} & \rho_{33} \\
    \end{bmatrix}
  =
    \begin{bmatrix}
                         900 &    0 \cdot 30 \cdot 10 &    0 \\
         0 \cdot 10 \cdot 30 &                    100 &    0 \\
                           0 &                      0 &   16 \\
    \end{bmatrix}
  =
    \begin{bmatrix}
                         900 &                      0 &    0 \\
                           0 &                    100 &    0 \\
                           0 &                      0 &   16 \\
    \end{bmatrix}
\]

\paragraph{(iii) $\rho_{12} = 0.8$, $\rho_{13} = \rho_{23} = 0$}
\[
  \Sigma\left(\textbf{X}\right) =
    \begin{bmatrix}
      \rho_{11} & \rho_{12} & \rho_{13} \\
      \rho_{21} & \rho_{22} & \rho_{23} \\
      \rho_{31} & \rho_{32} & \rho_{33} \\
    \end{bmatrix}
  =
    \begin{bmatrix}
                         900 &  0.8 \cdot 30 \cdot 10 &    0 \\
       0.8 \cdot 10 \cdot 30 &                    100 &    0 \\
                           0 &                      0 &   16 \\
    \end{bmatrix}
  =
    \begin{bmatrix}
                         900 &                    240 &    0 \\
                         240 &                    100 &    0 \\
                           0 &                      0 &   16 \\
    \end{bmatrix}
\]
This scenario should have the \textit{highest probability} of a top-tier
character, because the two values are positively correlated. That is, they both
tend to be either high or low at the same time.

\problem{1 (d)}
Output from R using \texttt{rmvnorm}:
\VerbatimInput{problem1d.out}
As we can see from the output, we seem to have reasoned correctly in problem 1
(c) about which scenarios should have the highest and lowest probability of
producing a top-tier character.

\clearpage
\problem{2 (a)}
The transform method in the lectures seem to first have been presented by
Çinlar in \cite[p.~96]{cinlar}. While I do not have the book, the key theorem 
is reproduced in \cite{generating}. I have quoted that verbatim as
theorem \ref{theorem:cinlar} \vpageref[below]{theorem:cinlar}.

~\begin{theorem}
  \label{theorem:cinlar}
  Let $\Lambda(t), t \geqslant 0$ be a positive-valued, continuous,
  nondecreasing function. Then the random variables $T_1, T_2, \dots$ are event
  times corresponding to a nonhomogeneous Poisson process with expectation
  function $\Lambda(t)$ if and only if $\Lambda(T_1), \Lambda(T_2), \dots$ are
  the event times corresponding to a homogeneous Poisson process with rate one.
\end{theorem}

In other words, it relates a \textit{homogeneous} Poisson process with a
\textit{constant rate of one} to a non-homogeneous Poisson process. We can
actually go backwards by sampling event times from the homogeneous, rate 1
Poisson process, then go backwards through $\Lambda^{-1}$ to then generate
samples from the non-homogeneous process. See algorithm \vref{algorithm:nhpp}.

In algorithm \vref{algorithm:nhpp}, $\Lambda^{-1}$ is the inverted rate
function, while $\textrm{rpois}$ is a function for generating numbers from a
homogeneous Poisson. The algorithm operates in a for-loop. In the
implementation, we will operate on vectors.

Note that we transform the domain $[a,b]$ to HPP-space by using $\Lambda$.

I have also tested Çinlar's method from \cite{generating}
and found it to be equivalent to algorithm
\ref{algorithm:nhpp}, although more compressed. The details are in algorithm
\vref{algorithm:cinlar}. It also seems to use an algorithm by Donald Knuth
\cite{knuth} to sample numbers from the Poisson distribution. A plot using
Çinlar's method is given in plot \vref{plot:cinlar}.

Finally, I have compared all of this with the code from the lecturer using the
thinning method, shown in plot \vref{plot:thinning}.

\begin{algorithm}
  \caption{Generates $n$ numbers for the non-homogeneous Poisson process (NHPP)}
  \label{algorithm:nhpp}
  \begin{algorithmic}[1]
    \Function{rhpp}{$\lambda, a, b$}
    \Comment{Homogeneous Poisson process sampler}
      \Let{$n$}{$3 (b-a) \lambda$} \Comment{Number of samples, arbitrary 3}
      \Let{$w$}{$a + \textrm{cumsum}(\textrm{rexp}(n, \textrm{rate}=\lambda))$}
        \Comment{Cum. sum of samples from exp. distribution}
      \State \Return{$w$ so that $a \leqslant w \leqslant b$}
    \EndFunction
    \Function{rnhpp}{$n, \Lambda^{-1}$}
      \Comment{Non-homogeneous Poisson process sampler}
      \Let{$w$}{\Call{rhpp}{$1, \Lambda(a), \Lambda(b)$}}
        \Comment{Transforms $a$ and $b$ to HPP-space}
      \Let{$s$}{vector of length $|w|$}
      \For{$i \gets 1 \textrm{ to } |w|$}
        \Let{$s_i$}{$\Lambda^{-1}(w_i)$}
      \EndFor
      \State \Return{$s$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Çinlar's method for NHPP}
  \label{algorithm:cinlar}
  \begin{algorithmic}[1]
    \Function{cinlar}{$n$}
      \Let{$s$}{$0$}
      \Let{$t$}{vector of length $n$}
      \For{$i \gets 1 \textrm{ to } n$}
        \Let{$u$}{$U[0,1]$} \Comment{Uniform sample}
        \Let{$s$}{$s - \log{(u)}$}
        \Let{$t_i$}{$\Lambda^{-1}(s)$}
      \EndFor
      \State \Return{$t$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\paragraph{Finding $\Lambda$ and $\Lambda^{-1}$.}
In practice, this means that the rate function $\Lambda(t)$ must be readily
reversible. That may not always be the case, but suffices for the current task.
We will start by finding it.

The intensity is given by $\lambda(t) = 14t^{0.4}$. We then have that
\begin{align*}
  \Lambda(t) &= \int_0^t{\lambda(u)}\, \textrm{d}u
    =  14\int_0^t{u^{0.4}}\, \textrm{d}u
    = \frac{14}{1.4}\left[ u^{1.4} \right]_0^t = 10t^{1.4} = 10t^{\frac{7}{5}}
\end{align*}
If we set $w = \Lambda(t)$, its inverse $\Lambda^{-1}(w) = t$ is given by
\begin{align*}
  w &= 10t^{\frac{7}{5}} \\
  w^{\frac{5}{7}} &= 10^{\frac{5}{7}}t \\
  \Lambda^{-1}(w) &= t = 10^{-\frac{5}{7}} w^{\frac{5}{7}}
\end{align*}

\paragraph{Sampling from the entire period $t \in [0,5]$}
We simply use the same strategy as used in the lecture notes (and accompanying
code) and put this functionality in the homogeneous Poisson process sampler.
We calculate the expected number of events, and multiply that with a number
(three in the submitted code). We then filter away values that are outside this
range.

\problem{2 (b)}
See the plot in figure \vref{plot:2b}. Not shown here is a comparison with the
thinning method as given by the lecturer. In my tests, the two plots looked
similar, with equivalent domain and range.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{problem2b.pdf}
  \caption{Plot for problem 2 (b).}
  \label{plot:2b}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{problem2b_cinlars_method.pdf}
  \caption{Plot of Çinlar's algorithm for non-homogeneous Poisson process
  (NHPP), for comparison.}
  \label{plot:cinlar}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{problem2b_thinning.pdf}
  \caption{Plot of NHPP using the thinning method for comparison (lecturer's code)}
  \label{plot:thinning}
\end{figure}

\problem{2 (c)}
In both \cite{generating,rizzo} we see that $\Lambda(t)$ is the expected number of
failures in the period $[0, t]$, or $\Lambda(t) = \E\left[ N(t) \right]$. So
the expected number of failures during the first year is simply
\[
  \Lambda(1) = 10
\]

The expected number of failures during the last year, i.e.~from year four to
five, will then be
\[
  \Lambda(5) - \Lambda(4) = 25.53865
\]
For the other calculations, I have used R with the following strategy: I
transformed $t$ from NHPP-space to HPP space using the $\Lambda$ function, then
I calculated the probabilities using the definition of the Poisson
distribution.

Please see the code for problem 2c.  The output is:
\VerbatimInput{problem2c.out}

\problem{2 (d)}
Output from R:
\VerbatimInput{problem2d.out}

\problem{2 (e)}
The NHPP process gives us arrival times (i.e.,~points in time when a pump
fails). For each of those we can draw a sample from the gamme distribution with
shape $\alpha = 2$ and scale $\beta = 0.01$.

With that, we can do a number of things. We can calculate the total repair time
for the entire five year period. This aggregate should obviously be less than
five years, or else we will not be able to repair all the pumps within the
period.

\problem{2 (f)}
See the plot in figure \vref{plot:2f}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{problem2f.pdf}
  \caption{Plot for problem 2 (f).}
  \label{plot:2f}
\end{figure}

\problem{2 (g)}
Blank answer.

\problem{2 (h)}
Blank answer.

\problem{2 (i)}
Blank answer.

\clearpage
\problem{3 (a)}
% TODO: Add refs to Cinderalla book and Rizzo
The basic idea of Monte Carlo integration is to approximate an integral by
sampling. By drawing random numbers in the plane, we can easily determine if
the point falls above or below the integrand line by inserting the random $x$
value in the integrand function. The integral, or area under the curve, should
then be approximated with the ratio of points that fall above and below the
line.

\paragraph{Approximating the integral with crude Monte Carlo integration.}

This will be a win in situations where the integral is impossible, hard or
computationally expensive to perform --- this applies especially for
multi-dimensional integrals, where Monte Carlo integration scales linearly
while true integration scales polynomially.

The \textit{crude Monte Carlo integration} (CMC) is given by
\[
  \hat{\theta}_{\textrm{CMC}} = \frac{b-a}{n} \sum_{i=1}^{n} g(X_i) =
  (b-a)\overline{g(X)}
\]
Here, $f(x) = \frac{1}{b-a}$ is the density function, and $n$ is the number of
samples drawn from $X \sim U[a,b]$. By the law of large numbers,
$\hat{\theta}_{\textrm{CMC}} \to \theta$ as $n \to \infty$. The derivation of the above
equation is given in the lecture notes.

\paragraph{Estimating the number of simulations.}

To have a $(1 - \alpha) 100\%{}$ probability that $\hat{\theta}_{\textrm{CMC}}$
is at most a margin $e$ from $\theta$ we need use the empirial standard
deviation equation.
\[
  SD(\hat{\theta}_{\textrm{CMC}}) = \frac{(b - a)\sigma_{g(X)}}{\sqrt{n}}
\]
which gives us
\[
  n > \frac{z_{\sfrac{\alpha}{2}}^2 (b - a)^2 \sigma_{g(x)}^2 }{e^2}
    = \sigma_{g(x)}^2
        \left(
            \frac{z_{\sfrac{\alpha}{2}}(b - a)}{e}
          \right)^2
\]
We can approximate $\sigma_{g(X)}^2$ as well with
\[
  \widehat{\sigma_{g(X)}^2} = \frac{1}{1 - n} \sum_{i=1}^n \left(
        g(X_i) - \overline{g(X)} \right)^2
\]
where $X \sim [a, b]$.

This gives us a probabilistic lower bound for $n$ so that
$\hat{\theta_{\textrm{CMC}}}$ deviates no more than $e = 100$ from the true
$\theta$ in $(1 - \alpha)100 = 95\%{}$ of the time. Of course, this value will
depend heavily on accuracy of $\widehat{\sigma_{g(X)}}$ and thus on the number
of samples used to approximate it.

I have implemented this estimation in the code for problem 3 (b)
\vpageref{problem:3b}.

\problem{3 (b)}
\label{problem:3b}
Output from R:
\VerbatimInput{problem3b.out}
In the above program, we also tested the choice of $\hat{n}$ (\texttt{n.hat},
the number of simulations to find a good $\hat{\theta}$) over a number of runs.
According to confidence intervals, $100(1 - \alpha)\%{}$ of the time,
$\hat{\theta}$ should fall within $e = 100$ of the actual $\theta$.

\problem{3 (c)}
The \textit{hit or miss Monte Carlo integration} (HM) is done by choosing a
constant $c$ so that $0 \leqslant g(x) \leqslant c$ for $x \in [a,b]$. We then
draw random points in this area with $X \sim U[a, b]$ and $Y \sim U[0, c]$ and
keep track of how many hit inside the area delimited by
$g(x)$ and $y=0$.

This should then enable us to approximate $\theta = \int_a^b
g(x)\, \textrm{d}x$. The number of hits should be equal to
$\E\left[ \I(Y \leqslant g(x)) \right]$ and the rectangular area is given by
$c(b-a)$, or
\[
  \theta = \int_a^b g(x)\, \textrm{d}x
    = c(b-a)\, \E\left[ \I(Y \leqslant g(x)) \right]
\]
or
\[
  \hat{\theta}_{\textrm{HM}} =
    \frac{c(b-a)}{n} \sum_{i=1}^n \I(Y_i \leqslant g(X_i))
\]

\paragraph{Choosing a suitable constant $c$.}
We need to find a suitable $c > \lambda(t)$ for $t \in [a, b]$. This can be
done in R itself (where we use $g(x) = \lambda(t)$) by using
\begin{Verbatim}
t <- optimize(g, interval=c(a, b), maximum=TRUE)
c <- ceiling(g(t$maximum))
\end{Verbatim}
This gives us $c = 1052$.

\paragraph{Estimating the number of simulations.}
The estimated standard error for HM is given by
\[
  \widehat{\textrm{SD}}(\hat{\theta}_{\textrm{HM}})
    = c(b-a) \frac{ \sqrt{\hat{p}(1-\hat{p})}  }{\sqrt{n}}
\]
where $p = \Prb(Y \leqslant g(X))$ is estimated with
\[
  \hat{p} = \frac{1}{n} \sum_{i=1}^{n} \I( Y_i \leqslant g(X_i) )
\]
That means we can find the number of $n$ simulations required to be less than
$e$ away from the true $\theta$ with a $(1-\alpha)$ confidence interval by
\begin{gather*}
  z_{\sfrac{\alpha}{2}}\, \widehat{\textrm{SD}}(\hat{\theta}_{\textrm{HM}}) < e \\
  z_{\sfrac{\alpha}{2}}\, c(b-a) \sqrt{ \frac{\hat{p}(1-\hat{p})}{n} } < e \\
  n > \hat{p} (1 - \hat{p})
          \left( \frac{ z_{\sfrac{\alpha}{2}}\, c(b - a) }{ e } \right)^2
\end{gather*}

We can now estimate $\hat{p}$ using using $\alpha = (1-0.95) = 0.05$ and $e =
100$. Output from R:
\VerbatimInput{problem3c.out}

As we can see from the output\footnote{The R output is always run
whenever I reproduce this PDF document. Therefore, each run will always be
slightly different.},
we will require around 60 thousand runs to estimate
$\hat{\theta}_{\textrm{HM}}$ with the given confidence. That is much larger
than what we found for the crude Monte Carlo (CMC), which required 18--20
thousand runs. The reason is that in HM, the underlying information is
compressed into a strictly binary outcome (i.e.,~zero or one), which throws away much of
the information. We then take the mean of the binary values, losing information
like, for example, how far away the sampled point was from $\lambda(t)$. Thus,
getting a good mean of the binary values simply requires many more of them.

Finally, the assignment asks for an \textit{upper} bound on $n$. This may be a
semantic issue, but I would believe that what I have calculated is a
\textit{lower} bound: The least $n$ that gives us the desired confidence
interval.

%%% TODO:
% Upper bound (isn't it lower bound?). Anway, can clearly see whith requires
% the least number of simulations. But can also compare the variances to see
% how effective they are. I.e., if the var1/var2 ratio < 1, then var1 is more
% efficient, because then it is lower (less variance), and that's exactly what
% we want to have. Also, can we find the variance explicitly? That would be
% nice, but probably difficult for the integrand given.
%
% Lectures say that w/same number of iterations, CMC is more efficient, because
% HM reduces data to 0 and 1. We can repeat that, but better yet to include
% more information.
%
% SEE CHAPTER @ for the confidence intervals! I think there is important info
% there! Also, precision increases with sqrt(n), so need do DOUBLE n to get
% more precision (or even more than that)...

\problem{3 (d)}
Output from R:
\VerbatimInput{problem3d.out}

\clearpage
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,assignment-2}

\end{document}
