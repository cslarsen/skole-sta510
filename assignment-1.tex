\documentclass[a4paper,english,12pt]{article}
\input{preamble}

\title{STA-510 Statistical modelling and simulation}
\subtitle{Mandatory exercise 1}
\author{Christian Stigen}
\date{UiS, September \nth{21}, 2017}

\begin{document}
\maketitle

\problem{1 (a)}
The probability density function is given in \cite{walpole} as
\[
  f(x) =
    \begin{cases}
      \displaystyle
        \frac{1}{\beta} e^{-\frac{x}{\beta}} & x > 0  \\
        0 & \text{elsewhere}
    \end{cases}
\]
We then have that
\[
  \Prb(X \leqslant x) = \int_{-\infty}^{x} f(t)\, \mathrm{d}t 
    = \left[ -e^{-\frac{t}{\beta}} \right]_{0}^{x} = 1 - e^{-\frac{x}{\beta}}
\]

\problem{1 (a i)}
\[
  \Prb(X > 4000) = 1 - \Prb(X \leqslant 4000) = e^{-\frac{4000}{5000}} \approx 0.449
\]

\problem{1 (a ii)}
\[
  \Prb(4000 \leqslant X \leqslant 6000) =
    \Prb(X \leqslant 6000) - \Prb(X \leqslant 4000) =
       e^{-\frac{4000}{5000}} - e^{-\frac{6000}{5000}}
       \approx 0.148
\]
Note that for continuous density functions, $\Prb(X=x) = 0$. Therefore I have not
been diligent in the use of the less-than-or-equal symbols here.

\problem{1 (b)}
Output from R:
\VerbatimInput{problem1b.out}

\problem{1 (c)}
Output from R:
\VerbatimInput{problem1c.out}
We can clearly see that more samples gets us closer to the computed value.

\problem{1 (d)}
Output from R:
\VerbatimInput{problem1d.out}

\problem{1 (e)}
Output from R:
\VerbatimInput{problem1e.out}

\problem{2 (a)}
For each round, we draw seven random numbers from 1--34. We count how many
draws contain at least two consecutive number ($c$) and divide by how many
total draws we have made ($n$). The probability will then be
\[
  \Prb(\text{contains two or more consecutive numbers}) = \frac{c}{n}
\]
% TODO: Find how many sims to be 95% that the error is at most 0.01. See slides
% for this!
We have
\begin{align*}
  \text{ME} &= z\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \\
  \left(\frac{\text{ME}}{z}\right)^2 &= \frac{\hat{p}(1-\hat{p})}{n} \\
  n &= \ceil*{ \hat{p}(1-\hat{p}) \frac{z^2}{\text{ME}^2} }
\end{align*}
where $\ceil{\dots}$ is the ceiling function.

I wrote a test program that seems to indicate that $\hat{p} = 0.779$. The
z-value must be 1.96 for a 95\%{} interval, and we have the margin of error
$\text{ME} = 0.01$, which gives us
\[
  n = 6614
\]
That suggests that we should have around 6614 samples.

\problem{2 (b)}
Output from R:
\VerbatimInput{problem2b.out}

\problem{3 (a)}
\label{problem:3a}
\paragraph{Problem} We want to sample x-values whose distribution is given by a
probability density function (PDF) is $f(x)$.

\paragraph{Solution} We use the \textit{inverse transform method}
\cite{wiki:inverse.transform.method}: Since any the values given by the PDF is
within 0--1, we use a uniform pseudo-random number generator to select a value
in the interval 0--1. We then find which x-value will give this value by
finding the inverse of the cumulative density function (CDF) $g$.

The CDF is given by
\begin{align*}
  g(x) & = \int_{-\infty}^{x} f(x)\, \mathrm{d}t 
     = \left[ -e^{-\frac{t^2}{2\theta^2}} \right]_0^{x} = 
     1 - e^{-\frac{x^2}{2\theta^2}}, \text{ where } x>0 \text{ and } \theta > 0
\end{align*}
We now need to find $g^{-1}(u)$ for
\begin{align*}
  g(g^{-1}(u)) &= 1 - e^{-\frac{g^{-1}(u)^2}{2\theta^2}} = u \\
   e^\frac{g^{-1}(u)^2}{2\theta^2}  &= \frac{1}{1 - u} \\
   g^{-1}(u)^2  &= 2\theta^2\log{\left(\frac{1}{1 - u}\right)} \\
   g^{-1}(u)  &= \pm\theta\sqrt{2\log{\left(\frac{1}{1 - u}\right)}} \\
\end{align*}
When I first solved this, I got the expression $\theta\sqrt{-2\log{(1-u)}}$ and
thought I had made a mistake --- because of the minus sign. However, since
$u<1$, the logarithm expression will obviously always compute to a negative
number. That is, the following two expressions are exactly the same:
\[
  \theta\sqrt{-2\log{(1-u)}} = \theta\sqrt{2\log{\left(\frac{1}{1-u}\right)}}\,
  \text{ where } u \in \left[0,1\right>
\]
When implemented on a computer, the left expression \textit{could} be
numerically more stable, depending on the floating point system used. With IEEE
754 (which is most likely the one that R uses), it doesn't seem to matter.
Anyway, it avoids one division. In fact, this is the one that was chosen in the
\texttt{rrayleigh} implementation in the \texttt{VGAM} package\footnote{In
\texttt{VGAM}, the $\theta$-parameter is called \textit{scale}.}
\cite{github:vgam}.

As to the $\pm$ operator, the whole expression returns a value for $x$, and
since we know $x>0$ we can remove the minus sign. We then get
\begin{align*}
  x(u) &= g^{-1}(u) = \theta\sqrt{2\log{\left(\frac{1}{1 - u}\right)}}
   \text{ where } x>0\, , \theta>0\, , u \in \left[0,1\right>
   \\
\end{align*}%
The algorithm to generate samples from the Rayleigh distribution is given in
algorithm \vref{algorithm:rayleigh}. Note that in the actual implementation I
made for the Rayleigh sampling, I used the \texttt{runif} function to draw
uniform floating point samples. This function does \textit{not} include its
extreme values, meaning you only get $\left<0,1\right>$ in the default case.
This is explained in the help for \texttt{runif}. I originally used
\texttt{sample(1:10\^{}decimals-1, n) / 10\^{}decimals} to generate
$\left[0,1\right>$, but that was painstakingly slow compared to \texttt{runif}.

\begin{algorithm}
  \caption{Generates $n$ samples from the Rayleigh distribution}
  \label{algorithm:rayleigh}
  \begin{algorithmic}[1]
    \Function{rayleigh}{$n, \theta$}
      \Let{$\textrm{samples}$}{vector of size $n$}
      \For{$i \gets 1 \textrm{ to } n$}
        \Let{$u$}{$\mathrm{uniform}(0,1)$} \Comment{Random float $u \in \left[0,1\right>$}
        \Let{$\textrm{samples}[i]$}{$\theta\sqrt{2\log{\left(\sfrac{1}{1-u}\right)}}$}
      \EndFor
        \State \Return{samples}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\problem{3 (b)}
A histogram of Rayleigh samples is given in figure \vref{plot:3b}, while a
comparison between the expected value, variance and their sample counterparts
are given in table \vref{table:3b}.

\input{problem3b.out}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{problem3b.pdf}
  \caption{Rayleigh samples for problem 3 (b) with $\theta=1.78$. The expected
  value and PDF are indicated by the red, dotted lines.}
  \label{plot:3b}
\end{figure}

\problem{3 (c)}
One simulation: Set $\theta=1.3$ and take Rayleigh 200 samples. Return 0 if all
waves are less than or equal to five meters, 1 if not. We then perform a large
number of simulations and divide the sum of each simulation by the number of
simulations made.

I've been experimenting with an alternative way: Take $200n$ samples where $n$
is the number of simulations. Then count how many waves are larger than five
meters, and divide by $n$. The rationale is that each wave is independent, and
because of the law of large values, we should be able to ignore if any wave
occurs in a batch of 200 or not. While this is much faster to computer in R, it
does give just slightly different results. This is included in the output in
problem 3 (d) as \texttt{simulations v2}, but I prefer not to use that.

\problem{3 (d)}
\label{problem:3d}
Output from R:
\VerbatimInput{problem3d.out}

\problem{3 (e)}
Acceptance-rejection sampling was first presented in \cite{von1961various} by the
famous Jon von Neumann, and is a beautiful piece that every one should read.

... 

We are supposed to assume that the PDF of
the triangle distribution, or that it is impractical to sample from directly.
Instead, we should find a function with similar shape and find a constant $c$
so that $\frac{f(t)}{cg(t)} \leqslant c$ for all $t$. But since we need a
function that follows the shape of the triangular PDF itself, we must
reimplement it but with a factor off so that we can use acceptance-rejection
sampling. Instead, we just set $c=\frac{2}{(b-a)}$ to normalize $f$ so that we
check if $u < \sfrac{f(y)}{c}$.

This is a somewhat contrived case, and the sentiment is shared in
\cite{STEIN20091143}:

\textit{In the case of the triangular, this is not competitive with the
inversion method but is widely used for pedagogical purposes.}

Furthermore, the examples given in the lectures accepts only when $u$ is
\textit{less than}, but in \cite{stanford} they use
\textit{less-than-or-equals}. I have not looked more into that detail, but have
used less than or equals.

For details on how I have implemented this, see the code and algorithm
\ref{algorithm:rtriangle} on page \pageref{algorithm:rtriangle}.

\begin{algorithm}
  \caption{Generates $n$ samples from the triangle distribution}
  \label{algorithm:rtriangle}
  \begin{algorithmic}[1]
    \Function{ptriangle}{$x, a, b, c$}
      \If{$x \leqslant c$}
        \State \Return{$\frac{2}{(b-a)(c-a)}(x-a)$}
      \Else
        \State \Return{$\frac{2}{(b-a)(b-c)}(b-x)$}
      \EndIf
    \EndFunction
    \Function{rtriangle}{$n, a, b, c$}
      \Let{$c$}{$ \sfrac{2}{(b-a)}$}
        \Comment{Constant so that $\forall t: \frac{f(t)}{g(t)} \leqslant c$}
      \Let{$f(x)$}{\Call{ptriangle}{$x, a, b, c$}}
        \Comment{Partial application}
      \Let{$\textrm{samples}$}{numeric vector of size $n$}
      \For{$i \gets 1 \textrm{ to } n$}
        \Repeat
          \Let{$u$}{$\textrm{uniform}(0, 1)$}
          \Let{$y$}{$\textrm{uniform}(a, b)$} \Comment{$y \in \left[a,b\right]$}
        \Until{$u \leqslant \frac{f(y)}{c}$}
        \Let{$\textrm{samples}[i]$}{$y$}
      \EndFor
      \State \Return{samples}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\problem{3 (f)}
For a plot of algorithm \vref{algorithm:rtriangle}, see figure \vref{plot:3f}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{problem3f.pdf}
  \caption{Acceptance-rejection sampling from the triangle distribution with
  $a=1.5$, $b=3$ and $c=2$.}
  \label{plot:3f}
\end{figure}

\problem{3 (g)}
The probability that a \textit{single} wave is smaller or equal to $y$ is given
by the CDF in problem 3 (a) on page \pageref{problem:3a}:
\[
  \Prb(X\leqslant y) = 1 - e^{-\frac{y^2}{2\theta^2}}
\]
The probability that $m$ waves are smaller than or equal to $y$ will then be
\[
  \Prb(Y \leqslant y)
    = \Prb(X\leqslant y)^m
    = \left( 1 - e^{-\frac{y^2}{2\theta^2}} \right)^m
\]
because each wave is independent of another (although likely untrue in real
life).

Now, the reverse would be that \textit{at least one} wave is higher than $y$.
We can express that as
\[
  \Prb(Y > y)
    = 1 - \Prb(Y \leqslant y)
    = 1 - \left(1 - e^{-\frac{y^2}{2\theta^2}} \right)^m
\]
$\hfill\blacksquare$

The exact calculation with $\theta=1.3$ and $m=200$ gives
\[
  \Prb(Y>5)
    = 1 - \left(1 - e^{\frac{-y^2}{2\theta^2}} \right)^m
    = 1 - \left(1 - e^{\frac{-5^2}{2\cdot1.3^2}} \right)^{200}
    = 0.11549135704657076\dots
\]
This corresponds quite well with the output in problem 3 (d) on page
\pageref{problem:3d}.

\problem{3 (h)}
\problem{3 (i)}

\clearpage
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
