\documentclass[a4paper,english,12pt]{article}
\input{preamble}

\title{STA510 --- Statistical modelling and simulation}
\subtitle{Exercise Set 1}
\author{Christian Stigen}
\date{UiS, September \nth{21}, 2017}

\begin{document}
\maketitle

\problem{1 (a)}
The probability of getting \textit{any specific} sequence is exactly the same,
because if we list all possible outcomes for $n$ flips, each sequence will
appear exactly once.

It is understood that each flip is stochastic, meaning there is no memory
involved. Thus, each flip is independent of any other flips. That the coin is
unbiased means that the probability for getting heads or tails is exactly
$0.5$, i.e.~the outcome is both binary and evenly distributed. In fact, this is
called a \textit{Bernoulli process}, and each flip is a \textit{Bernoulli
trial}.

In mathematical terms, we have that
\begin{align*}
  P(\textrm{HHTHT}) = P(\textrm{THHHT}) = P(\textrm{five flips}) =
    \left(\frac{1}{2}\right)^5 = \frac{1}{32}
\end{align*}
Conversely, we see that there are $2^5 = 32$ possible ways to toss a fair coin
five times.

\problem{1 (b)}
I modified the lecture code to simulate the number of heads in five coin flips.
The relative frequencies for different number of simulations are shown in
figure \vref{plot.1b}.

As the number of simulations increase, we can see that the plot seem to
approximate the normal distribution. This is what we expect from a discrete,
Binomial distribution.
\begin{figure}[h]
  \includegraphics[width=\textwidth]{code1/problem-1b.pdf}
  \caption{Plot for problem 1 (b) on page \pageref{problem.1 (b)}}
  \label{plot.1b}
\end{figure}

\problem{1 (c)}
I added the keyword argument \texttt{prob=c(0.4, 0.6)} to the \texttt{sample}
function. That makes the probabilities for getting heads and tails 0.6 and 0.4,
correspondingly.

The resulting plots can be seen in figure \vref{plot.1c}. We clearly see that
for a large number of samples, the plots are skewed to the right, as we expect.
\begin{figure}[h]
  \includegraphics[width=\textwidth]{code1/problem-1c.pdf}
  \caption{Plot for problem 1 (c) on page \pageref{problem.1 (c)}}
  \label{plot.1c}
\end{figure}

\problem{2 (a)}
The definition for a probability mass function (PMF) require that
\begin{align}
  f(x) &\geqslant 0 \label{pmf.1} \\
  \sum_x{f(x)} &= 1 \label{pmf.2} \\
  f(x) &= P(X=x) \label{pmf.3}
\end{align}
For $f(x) = x/c$ with $x=1,2,3,4$, we see that requirement (\ref{pmf.1}) is met.
But for (\ref{pmf.2}) and (\ref{pmf.3}), we must have
$\sfrac{1}{c} + \sfrac{2}{c} + \sfrac{3}{c} + \sfrac{4}{c} = \sfrac{10}{c}$
or $c=10$.

The bar plot can be seen in figure \vref{plot.2a}.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{code1/problem-2a.pdf}
  \caption{Plot for problem 2 (a) on page \pageref{problem.2 (a)}}
  \label{plot.2a}
\end{figure}

\problem{2 (b)}
For $f(x) = c(x+1)^2$ with $x=0,1,2,3$ we have the sum $c+4c+9c+16c=30c$.  To
make the sum one, we must therefore have $c=\frac{1}{30}$. All other
requirements are then met.

The bar plot can be seen in figure \vref{plot.2b}.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{code1/problem-2b.pdf}
  \caption{Plot for problem 2 (b) on page \pageref{problem.2 (b)}}
  \label{plot.2b}
\end{figure}

\problem{3}
Since we have a finite number of values, the expected value $E(X)$ is given by
\[
  E[X] = x_1p_1 + x_2p_2 + \cdots + x_kp_k
\]
where $x_n$ is the outcome of the random variable, while $p_n$ is its
probability. Since we already know $c$ in both cases, we can just plug the
numbers in.

\paragraph{The expected value for problem 2 (a)} is given by
\[
  E(X) =
      1\frac{1}{c} +
      2\frac{2}{c} +
      3\frac{3}{c} +
      4\frac{4}{c} +
      = \frac{30}{c}
\]
With $c=10$ we then get $E(X) = 3$.

Likewise, $E(g(X))$ with $g(x)=x^3$ and $c=10$ gives
\[
  E(g(X)) =
    1 \frac{1^3}{1000} +
    2 \frac{2^3}{1000} +
    3 \frac{3^3}{1000} +
    4 \frac{4^3}{1000} =
    \frac{1 + 16 + 81 + 256}{1000} = \frac{354}{1000} = 3.54
\]

\paragraph{The expected value for problem 2 (b)} is given by
\[
  E(X) =
    0 +
    1 c(1+1)^2 +
    2 c(2+1)^2 +
    3 c(3+1)^2 =
    4c + 18c + 48c = 70c
\]
With $c=\frac{1}{30}$ we get $E(x) = \frac{7}{3} \approx 2.33$. For $E(g(X))$,
as above, we get
\begin{align*}
  E(g(X)) &=
    0 +
    1 c^3(1+1)^6 +
    2 c^3(2+1)^6 +
    3 c^3(3+1)^6 \\
    &= c^3(64 + 1458 + 12288) = 13747c^3 \\
    &= \frac{13810}{27000} \approx 0.5115
\end{align*}

To double-check that the above values are indeed correct, I wrote a small R
program to find $E(X)$ through iterated weighted sampling. It can be seen in
figure \vref{code.3}.
%
An example run produced the output
\verbatiminput{code1/problem-3.out}
which corresponds very well with our calculated expected values, except for the
last $E(g(X)) = 2.8232$. This has to do with how the code is structured: It
uses the \texttt{sample} function, but provides a set of weighs that don't sum
up to $1.0$. For the first $E(g(x))$, the values happen to sum to one.
But we can normalize the second one by just summing the weighs. They sum to
0.1811, so if we normalize we get $2.8282\sfrac{1}{0.1811} = 0.5122$, which is
close enough to $0.5115$\footnote{Note that the R output here is regenerated each time
this \LaTeX document is rendered, so the exact figures may vary.}

\begin{figure}[t]
  \verbatiminput{code1/problem-3.R}
  \caption{R program for simulating the expected value of a PMF (problem 3)}
  \label{code.3}
\end{figure}


\problem{4 (a)}
\problem{4 (b)}
\problem{4 (c)}

\problem{5 (a)}
To determine $k$, we use the fact that any probability density function (PDF)
must have an area of exactly one (of course without any negative
probabilities). Since $f(x) = 0$ for $|x| > 1$, we only have to solve
\begin{align*}
  \int_{-1}^{1}{k(1-x^2)~dx} &= 1 \\
  k\left[ x - \frac{1}{3}{x^3} \right]_{-1}^{1} =
  k\left( 1 - \frac{1}{3} + 1 - \frac{1}{3} \right) = \frac{4}{3}k &= 1 \\
   k &= \frac{3}{4}
\end{align*}

\problem{5 (b)}
\problem{5 (c)}

\problem{6 (a)}
\problem{6 (b)}
\problem{6 (c)}

\problem{7 (a)}
\problem{7 (b)}

\problem{8}

\problem{9 (a)}
\problem{9 (b)}

\problem{10 (a)}
\texttt{\input{code1/problem-10a.R}} gives
\texttt{\input{code1/problem-10a.out}}

\problem{10 (b)}
\texttt{\input{code1/problem-10b.R}} gives
\texttt{\input{code1/problem-10b.out}}

\problem{10 (c)}
\verbatiminput{code1/problem-10c.R} gives
\texttt{\input{code1/problem-10c.out}}

\problem{10 (d)}
\texttt{\input{code1/problem-10d.R}} gives
\texttt{\input{code1/problem-10d.out}}

\problem{11 (a)}

\end{document}
