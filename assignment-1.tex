\documentclass[a4paper,english,12pt]{article}
\input{preamble}

\title{STA-510 Statistical modelling and simulation}
\subtitle{Mandatory exercise 1}
\author{Christian Stigen}
\date{UiS, September \nth{21}, 2017}

\begin{document}
\maketitle

% List of TODO:
% - Problem 2a: Estimate hat(p) empirically, see how they do it in the slides
%   (maximum likelihood estimation?)
% - Problem 3e: finding number of expected iterations
% - Problem 3h: Look in the slides

\section*{\normalsize{How to run the code}}
All the code for this exercise can be found in \texttt{assignment-1.R}. I have
made a single function for each problem. For example, to see the output of
problem 1 (d), execute the function \texttt{problem1d()}.  From the UNIX
command-line, you can run
\begin{verbatim}
Rscript -e 'pdf("problem1b.pdf");
            source("assignment-1.R");
            problem1b()' > problem1b.out
\end{verbatim}
to produce the textual output file \texttt{problem1b.out} and
any plots in \texttt{problem1b.pdf}.

\problem{1 (a)}
The probability density function is given in \cite{walpole} as
\[
  f(x) =
    \begin{cases}
      \displaystyle
        \frac{1}{\beta} e^{-\frac{x}{\beta}} & x > 0  \\
        0 & \text{elsewhere}
    \end{cases}
\]
We then have that
\[
  \Prb(X \leqslant x) = \int_{-\infty}^{x} f(t)\, \mathrm{d}t 
    = \left[ -e^{-\frac{t}{\beta}} \right]_{0}^{x} = 1 - e^{-\frac{x}{\beta}}
\]

\problem{1 (a i)}
\[
  \Prb(X > 4000)
    = 1 - \Prb(X \leqslant 4000)
    = e^{-\frac{4000}{5000}}
    = 0.44932896\dots
\]

\problem{1 (a ii)}
\[
  \Prb(4000 \leqslant X \leqslant 6000) =
    \Prb(X \leqslant 6000) - \Prb(X \leqslant 4000) =
       e^{-\frac{4000}{5000}} - e^{-\frac{6000}{5000}}
       \approx 0.148
\]
Note that for continuous density functions, $\Prb(X=x) = 0$. Therefore I have not
been diligent in the use of the less-than-or-equal symbols here.

\problem{1 (b)}
Output from R using \texttt{pexp}:
\VerbatimInput{problem1b.out}

\problem{1 (c)}
Finding $\Prb(X > 4000)$ with simulations. Output from R:
\VerbatimInput{problem1c.out}
We can clearly see that as we increase the number of simulations, the
probability converges to the exact value of $0.4493\cdots$. However, it does
converge somewhat slowly.

\problem{1 (d)}
Using simulations to approximate the probability that the sum of the lifetimes
of two light bulbs is more than 10 thousand hours. Output from R:
\VerbatimInput{problem1d.out}

\problem{1 (e)}
Using the \texttt{pgamma} function in R gives the output:
\VerbatimInput{problem1e.out}
This is the exact solution for problem 1 (d). As we can see, our simulation
seems to give a good approximation.

\problem{2 (a)}
For each round, we draw seven random numbers from 1--34. We count how many
draws contain at least two consecutive number ($c$) and divide by how many
total draws we have made ($n$). The probability will then be
\[
  \Prb(\text{contains two or more consecutive numbers}) = \frac{c}{n}
\]
We have
\begin{align*}
  e &> z_{\sfrac{\alpha}{2}}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}} \\
  \left(\frac{e}{z_{\sfrac{\alpha}{2}}}\right)^2 &> \frac{\hat{p}(1-\hat{p})}{n} \\
  n &> \ceil*{ \hat{p}(1-\hat{p}) \frac{z_{\sfrac{\alpha}{2}}^2}{e^2} }
\end{align*}
where $\ceil{\dots}$ is the ceiling function and $e$ is the margin of
error.

I wrote a test program that seems to indicate that $\hat{p} = 0.779$. The
z-value must be 1.96 for a 95\%{} interval ($z_{0.025} = 1.96$), and we
have the margin of error $\text{ME} = 0.01$, which gives us
\[
  n = 6614
\]
That suggests that we should have \textit{at least} 6614 samples.

\problem{2 (b)}
Simulating the probability that seven out of 34 numbers contain at least two
consecutive numbers was done in R with the output:
\VerbatimInput{problem2b.out}

\problem{3 (a)}
\label{problem:3a}
We want to sample x-values whose distribution is given by a probability density
function (PDF) is $f(x)$.

We will use the \textit{inverse transform method}
\cite{wiki:inverse.transform.method}: Since the values given by the CDF is in
the range 0--1, we can select $u$ by drawing a number from a uniform
pseudo-random number generator. We then use the inverse CDF function
$g^{-1}(u)$ to find which $x$-value this corresponds to. This requires that it
is possible to invert the CDF.

The CDF is given by
\begin{align*}
  g(x) & = \int_{-\infty}^{x} f(x)\, \mathrm{d}t 
     = \left[ -e^{-\frac{t^2}{2\theta^2}} \right]_0^{x} = 
     1 - e^{-\frac{x^2}{2\theta^2}}, \text{ where } x>0 \text{ and } \theta > 0
\end{align*}
We now need to find $g^{-1}(u)$ for
\begin{align*}
  g(g^{-1}(u)) &= 1 - e^{-\frac{g^{-1}(u)^2}{2\theta^2}} = u \\
   e^\frac{g^{-1}(u)^2}{2\theta^2}  &= \frac{1}{1 - u} \\
   g^{-1}(u)^2  &= 2\theta^2\log{\left(\frac{1}{1 - u}\right)} \\
   g^{-1}(u)  &= \pm\theta\sqrt{2\log{\left(\frac{1}{1 - u}\right)}} \\
\end{align*}
Note that since $u<1$ then $\log{(1-u)} < 1$, meaning the following two
equations are exactly the same:
\[
  \theta\sqrt{-2\log{(1-u)}} = \theta\sqrt{2\log{\left(\frac{1}{1-u}\right)}}\,
  \text{ where } u \in \left[0,1\right>
\]
The left expression is how \texttt{rrayleigh} has been implemented in the
\texttt{VGAM} package \cite{github:vgam}. While it avoids one division, one
could imagine that it would be numerically more stable. I made a test program
using IEEE 754 floats \cite{1985--ieee754} (which is what R uses), and it
doesn't seem to be more stable.

As to the $\pm$ operator, the whole expression returns a value for $x$, and
since we know $x>0$ we can remove the minus sign. We then get
\begin{align*}
  x(u) &= g^{-1}(u) = \theta\sqrt{2\log{\left(\frac{1}{1 - u}\right)}}
   \text{ where } x>0\, , \theta>0\, , u \in \left[0,1\right>
   \\
\end{align*}%
The algorithm to generate samples from the Rayleigh distribution is given in
algorithm \vref{algorithm:rayleigh}. Note that in the actual implementation I
made for the Rayleigh sampling, I used the \texttt{runif} function to draw
uniform floating point samples. This function does \textit{not} include its
extreme values, meaning you only get $\left<0,1\right>$ in the default case.
This is explained in the help for \texttt{runif}. I originally used
\texttt{sample(1:10\^{}decimals-1, n) / 10\^{}decimals} to generate
$\left[0,1\right>$, but that was painstakingly slow compared to \texttt{runif}.

\begin{algorithm}
  \caption{Generates $n$ samples from the Rayleigh distribution}
  \label{algorithm:rayleigh}
  \begin{algorithmic}[1]
    \Function{rayleigh}{$n, \theta$}
      \Let{$\textrm{samples}$}{vector of size $n$}
      \For{$i \gets 1 \textrm{ to } n$}
        \Let{$u$}{$\mathrm{uniform}(0,1)$} \Comment{Random float $u \in \left[0,1\right>$}
        \Let{$\textrm{samples}[i]$}{$\theta\sqrt{2\log{\left(\sfrac{1}{1-u}\right)}}$}
      \EndFor
        \State \Return{samples}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\problem{3 (b)}
A histogram of Rayleigh samples is given in figure \vref{plot:3b}, while a
comparison between the expected value, variance and their sampled counterparts
is given in table \vref{table:3b}.

\input{problem3b.out}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{problem3b.pdf}
  \caption{Rayleigh samples for problem 3 (b) with $\theta=1.78$. The expected
  value and PDF are indicated by the red, dotted lines.}
  \label{plot:3b}
\end{figure}

\problem{3 (c)}
For one simulation: Set $\theta=1.3$ and take $200$ Rayleigh samples. Return
$0$ if all waves are less than or equal to $5$ meters, $1$ if not.

We then divide the total of a large number of such simulations with the number
of simulations made.

\problem{3 (d)}
\label{problem:3d}
Implemented the simulation described in problem 3 (c): The approximate
probability that highest wave is higher than 5 meters. Output form R:
\VerbatimInput{problem3d.out}

\problem{3 (e)}
Acceptance-rejection sampling was first presented in \cite{von1961various} by the
famous Jon von Neumann, and is a beautiful piece that every one should read.

Neumann argues that a good way to sample from a given distribution is to take
two uniformly sampled numbers, then feed one of them into a computationally
simpler function that is proportional to the desired PDF. By comparing with a
scale factor ($\alpha$ in the article, $c$ in the lecure notes), one can then
throw away the numbers if they are higher than the comparison operation. He
also shows how his algorithm enables to embed simpler mathematical expressions
directly in the code, if applicable.

However, using acceptance-rejection sampling for the triangle distribution is
somewhat contrived. We need to find a function whose shape is similar to that
of the actual PDF, and then use a scaling factor for comparison. However, the
function cannot be simplified further, so I've made a few tweaks. Those tweaks
are actually very much in line with von Neumann's original article, which is
something I want to stress: I just plug the pseudo-random number into the
actual PDF and use a normalization factor instead.

In \cite{STEIN20091143} we see that the \textit{inverse} of the triangle
distribution does not have a closed form, and therefore cannot be used with the
inverse transform method. The paper presents multiple alternatives, such as the
MINIMAX-method, but also presents a novel method of calculating the triangle
PDF in one line. However, I have used that formula in my code, because I wanted
to mimic the code given in the lectures.

It is also worth noting that \cite{von1961various, stanford} always use the
\textit{less-than-or-equal} comparison instead of \textit{less-than}.

For details on how I have implemented this, see the code and algorithm
\ref{algorithm:rtriangle} on page \pageref{algorithm:rtriangle}. Since we are
using the scaling factor $M = \sfrac{2}{(b-a)}$ (designated $c$ in algorithm
\ref{algorithm:rtriangle}), the expected number of iterations will be
%%% TODO: Number of iterations

\begin{algorithm}
  \caption{Generates $n$ samples from the triangle distribution}
  \label{algorithm:rtriangle}
  \begin{algorithmic}[1]
    \Function{ptriangle}{$x, a, b, c$}
      \If{$x \leqslant c$}
        \State \Return{$\frac{2}{(b-a)(c-a)}(x-a)$}
      \Else
        \State \Return{$\frac{2}{(b-a)(b-c)}(b-x)$}
      \EndIf
    \EndFunction
    \Function{rtriangle}{$n, a, b, c$}
      \Let{$c$}{$ \sfrac{2}{(b-a)}$}
        \Comment{Constant so that $\forall t: \frac{f(t)}{g(t)} \leqslant c$}
      \Let{$f(x)$}{\Call{ptriangle}{$x, a, b, c$}}
        \Comment{Partial application}
      \Let{$\textrm{samples}$}{numeric vector of size $n$}
      \For{$i \gets 1 \textrm{ to } n$}
        \Repeat
          \Let{$u$}{$\textrm{uniform}(0, 1)$}
          \Let{$y$}{$\textrm{uniform}(a, b)$} \Comment{$y \in \left[a,b\right]$}
        \Until{$u \leqslant \frac{f(y)}{c}$}
        \Let{$\textrm{samples}[i]$}{$y$}
      \EndFor
      \State \Return{samples}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\problem{3 (f)}
For a plot of algorithm \vref{algorithm:rtriangle}, see figure \vref{plot:3f}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{problem3f.pdf}
  \caption{Acceptance-rejection sampling from the triangle distribution with
  $a=1.5$, $b=3$ and $c=2$.}
  \label{plot:3f}
\end{figure}

\problem{3 (g)}
The probability that a \textit{single} wave is smaller or equal to $y$ is given
by the CDF in problem 3 (a) on page \pageref{problem:3a}:
\[
  \Prb(X\leqslant y) = 1 - e^{-\frac{y^2}{2\theta^2}}
\]
The probability that $m$ waves are smaller than or equal to $y$ will then be
\[
  \Prb(Y \leqslant y)
    = \Prb(X\leqslant y)^m
    = \left( 1 - e^{-\frac{y^2}{2\theta^2}} \right)^m
\]
because each wave is independent of another (although likely untrue in real
life).

Now, the reverse would be that \textit{at least one} wave is higher than $y$.
We can express that as
\[
  \Prb(Y > y)
    = 1 - \Prb(Y \leqslant y)
    = 1 - \left(1 - e^{-\frac{y^2}{2\theta^2}} \right)^m
\]
$\hfill\blacksquare$

The exact calculation with $\theta=1.3$ and $m=200$ gives
\[
  \Prb(Y>5)
    = 1 - \left(1 - e^{\frac{-y^2}{2\theta^2}} \right)^m
    = 1 - \left(1 - e^{\frac{-5^2}{2\cdot1.3^2}} \right)^{200}
    = 0.11549135704657076\dots
\]
This corresponds quite well with the output in problem 3 (d) on page
\pageref{problem:3d}, although I do notice that my algorithm requires a very
large number of samples to start converging to this value.

\problem{3 (h)}
Output from R:
\VerbatimInput{problem3h.out}

\problem{3 (i)}
We could sample $m$ from a distribution --- for example the normal
distribution, or perhaps the triangle distribution based on input from the
meteorologist and the weather forecast.

The next step would then be to repeat the experiment with different values for
$m$. An automatic recommendation could be generated for each experiment
(i.e.,~a binomial output) and summed together and divided by the number of
experiments. Or we could aggregate confidence intervals for each experiment and
then find out which interval would be most likely. This could be done by, for
example, finding overlapping regions (like in interval arithmetic) and
reporting on the most frequent. Or one could turn that into a frequency
distribution of some kind and look at that.

I feel that the best would be to base everything on actual, observable data,
like the weather forecast, consult with the opinion of the meteorologist,
consider details about the operation and facility (e.g.,~tear and wear of the
installation) and use that to make a more informed opinion.

\clearpage
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
