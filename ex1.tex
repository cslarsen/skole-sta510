\documentclass[a4paper,english,12pt]{article}
\input{preamble}

\title{STA510 --- Statistical modelling and simulation}
\subtitle{Exercise Set 1}
\author{Christian Stigen}
\date{UiS, September \nth{21}, 2017}

\begin{document}
\maketitle

\problem{1 (a)}
The probability of getting \textit{any specific} sequence is exactly the same,
because if we list all possible outcomes for $n$ flips, each sequence will
appear exactly once.

It is understood that each flip is stochastic, meaning there is no memory
involved. Thus, each flip is independent of any other flips. That the coin is
unbiased means that the probability for getting heads or tails is exactly
$0.5$, i.e.~the outcome is both binary and evenly distributed. In fact, this is
called a \textit{Bernoulli process}, and each flip is a \textit{Bernoulli
trial}.

In mathematical terms, we have that
\begin{align*}
  \P(\textrm{HHTHT}) = \P(\textrm{THHHT}) = \P(\textrm{five flips}) =
    \left(\frac{1}{2}\right)^5 = \frac{1}{32}
\end{align*}
Conversely, we see that there are $2^5 = 32$ possible ways to toss a fair coin
five times.

\problem{1 (b)}
I modified the lecture code to simulate the number of heads in five coin flips.
The relative frequencies for different number of simulations are shown in
figure \vref{plot.1b}.

As the number of simulations increase, we can see that the plot seem to
approximate the normal distribution. This is what we expect from a discrete,
Binomial distribution.
\begin{figure}[h]
  \includegraphics[width=\textwidth]{code1/problem-1b.pdf}
  \caption{Plot for problem 1 (b) on page \pageref{problem.1 (b)}}
  \label{plot.1b}
\end{figure}

\problem{1 (c)}
I added the keyword argument \texttt{prob=c(0.4, 0.6)} to the \texttt{sample}
function. That makes the probabilities for getting heads and tails 0.6 and 0.4,
correspondingly.

The resulting plots can be seen in figure \vref{plot.1c}. We clearly see that
for a large number of samples, the plots are skewed to the right, as we expect.
\begin{figure}[h]
  \includegraphics[width=\textwidth]{code1/problem-1c.pdf}
  \caption{Plot for problem 1 (c) on page \pageref{problem.1 (c)}}
  \label{plot.1c}
\end{figure}

\problem{2 (a)}
The definition for a probability mass function (PMF) require that
\begin{align}
  f(x) &\geqslant 0 \label{pmf.1} \\
  \sum_x{f(x)} &= 1 \label{pmf.2} \\
  f(x) &= \P(X=x) \label{pmf.3}
\end{align}
For $f(x) = x/c$ with $x=1,2,3,4$, we see that requirement (\ref{pmf.1}) is met.
But for (\ref{pmf.2}) and (\ref{pmf.3}), we must have
$\sfrac{1}{c} + \sfrac{2}{c} + \sfrac{3}{c} + \sfrac{4}{c} = \sfrac{10}{c}$
or $c=10$.

The bar plot can be seen in figure \vref{plot.2a}.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{code1/problem-2a.pdf}
  \caption{Plot for problem 2 (a) on page \pageref{problem.2 (a)}}
  \label{plot.2a}
\end{figure}

\problem{2 (b)}
For $f(x) = c(x+1)^2$ with $x=0,1,2,3$ we have the sum $c+4c+9c+16c=30c$.  To
make the sum one, we must therefore have $c=\frac{1}{30}$. All other
requirements are then met.

The bar plot can be seen in figure \vref{plot.2b}.

\begin{figure}[h]
  \includegraphics[width=\textwidth]{code1/problem-2b.pdf}
  \caption{Plot for problem 2 (b) on page \pageref{problem.2 (b)}}
  \label{plot.2b}
\end{figure}

\problem{3}
Since we have a finite number of values, the expected value $\E(X)$ is given by
\[
  \E(X) = x_1p_1 + x_2p_2 + \cdots + x_kp_k
\]
where $x_n$ is the outcome of the random variable, while $p_n$ is its
probability. Since we already know $c$ in both cases, we can just plug the
numbers in.

\paragraph{The expected value for problem 2 (a)} is given by
\[
  \E(X) =
      1\frac{1}{c} +
      2\frac{2}{c} +
      3\frac{3}{c} +
      4\frac{4}{c} +
      = \frac{30}{c}
\]
With $c=10$ we then get $\E(X) = 3$.

Likewise, $\E(g(X))$ with $g(x)=x^3$ and $c=10$ gives
\begin{align*}
  \E(g(X)) &= \sum_x{g(x)f(x)} =
    1^3\frac{1}{10} +
    2^3\frac{2}{10} +
    3^3\frac{3}{10} +
    4^3\frac{4}{10} =
    \frac{354}{10} = 35.4
\end{align*}

\paragraph{The expected value for problem 2 (b)} is given by
\begin{align*}
  \E(X) &=
    0 +
    1 c(1+1)^2 +
    2 c(2+1)^2 +
    3 c(3+1)^2 =
    4c + 18c + 48c = 70c \\
    &= \frac{70}{30} \approx 2.33
\end{align*}
For $\E(g(X))$, as above, we get
\begin{align*}
  \E(g(X)) &=
    0 +
    1^3 c(1+1)^2 +
    2^3 c(2+1)^2 +
    3^3 c(3+1)^2 \\
    &=  4c + 72c + 432c = \frac{508}{30} \approx 16.93
\end{align*}

To double-check that the above values are indeed correct, I wrote a small R
simulation to find $\E(X)$ through iterated weighted sampling, rather than
mechanically. It can be seen in figure \vref{code.3}.
%
An example run produced the output
\verbatiminput{code1/problem-3.out}
which corresponds very well with our calculated expected values.

While the simulation runs slower for this example, it will run \textit{vastly}
faster than the exact version when we have either a computationally expensive
probability mass function and a domain that covers a large number of discrete
values --- and even more so for complex, continuous probability density
functions.

\begin{figure}[t]
  \verbatiminput{code1/problem-3.R}
  \caption{R program for simulating the expected value of a PMF (problem 3)}
  \label{code.3}
\end{figure}

\problem{4 (a)}
We can calculate $\P(X < 0.5)$ from first principles by using its definition
\begin{equation}
  \label{eq.4a}
  \P(a < X < b) = \int_b^x{f(x)\, \mathrm{d} x}
\end{equation} and setting $a=-\infty$ and $b=0.5$, or
\begin{align*}
  \P(X < 0.5) &=
    \int_0^{0.5} 4x(1-x^2)\, \mathrm{d} x =
    \int_0^{0.5} 4x\, \mathrm{d} x - \int_0^{0.5} 4x^3\, \mathrm{d} x \\
  &=
    \left[2x^2 - x^4 \right]_0^{0.5}
  = \frac{1}{2} - \frac{1}{16} = \frac{7}{16} = 0.4375
\end{align*}
\paragraph{Normalization check}
\[
  \int_{-\infty}^{\infty} f(x)\, \mathrm{d} x =
    \left[ 2x^2 - x^4 \right]_{0}^{1} = 2 - 1 = 1
\]%
Since the area is $1$ and $f(x) \geqslant 0$ for all
$x$, we don't need to normalize $\P(X)$.

\paragraph{Expected value}
\[
  \E(X)
    = \int_{-\infty}^{\infty} x\,4x(1-x^2)\, \mathrm{d} x
    = \int_{0}^{0.5} 4x^2 - 4x^4 \, \mathrm{d} x
    = \left[ \frac{4}{3}x^3 - \frac{4}{5}x^5 \right]_{0}^{1}
    = \frac{8}{15} = 0.5333\dots
\]
\paragraph{Variance}
\begin{align*}
  \E(X^2)
    = \int_{-\infty}^{\infty} x^2\,4x(1-x^2)\, \mathrm{d} x
    = \int_{0}^{0.5} 4x^3 - 4x^5 \, \mathrm{d} x
    = \left[ x^4 - \frac{4}{6}x^6 \right]_{0}^{1}
    = 1 - \frac{4}{6} = \frac{1}{3}
\end{align*}
which gives us
\begin{align*}
  \Var(X) = \E(X^2) - \E(X)^2 = \frac{1}{3} - \frac{8^2}{15^2} =
    \frac{11}{225} = 0.04888\dots
\end{align*}
\paragraph{Standard deviation}\[
  \SD(X) = \sqrt{\Var(X)} = \sqrt{\frac{11}{225}} = \frac{\sqrt{11}}{15}
  \approx 0.2211
\]

\problem{4 (b)}

\problem{4 (c)}
A plot of $f$ and $F$ can be seen in figure \vref{plot.4c}.
\begin{figure}[h]
  \includegraphics[width=\textwidth]{code1/problem-4c.pdf}
  \caption{Plot for problem 4 (c) on page \pageref{problem.4 (c)}}
  \label{plot.4c}
\end{figure}

\problem{5 (a)}
To determine $k$, we use the fact that any probability density function (PDF)
must have an area of exactly one (of course without any negative
probabilities). Since $f(x) = 0$ for $|x| > 1$, we only have to solve
\begin{align*}
  \int_{-1}^{1}{k(1-x^2)~dx} &= 1 \\
  k\left[ x - \frac{1}{3}{x^3} \right]_{-1}^{1} =
  k\left( 1 - \frac{1}{3} + 1 - \frac{1}{3} \right) = \frac{4}{3}k &= 1 \\
   k &= \frac{3}{4}
\end{align*}

\problem{5 (b)}
\problem{5 (c)}

\problem{6 (a)}
\problem{6 (b)}
\problem{6 (c)}

\problem{7 (a)}
\problem{7 (b)}

\problem{8}

\problem{9 (a)}
\problem{9 (b)}

\problem{10 (a)}
\texttt{\input{code1/problem-10a.R}} gives
\texttt{\input{code1/problem-10a.out}}

\problem{10 (b)}
\texttt{\input{code1/problem-10b.R}} gives
\texttt{\input{code1/problem-10b.out}}

\problem{10 (c)}
\verbatiminput{code1/problem-10c.R} gives
\texttt{\input{code1/problem-10c.out}}

\problem{10 (d)}
\texttt{\input{code1/problem-10d.R}} gives
\texttt{\input{code1/problem-10d.out}}

\problem{11 (a)}

\end{document}
